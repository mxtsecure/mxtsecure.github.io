<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Xiangtao Meng | Trustworthy ML</title>
  <meta name="description" content="Researcher in trustworthy machine learning, focusing on deepfake forensics and LLM safety." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="assets/styles.css" />
</head>
<body data-theme="dark">
  <header class="site-header">
    <div class="logo">Xiangtao Meng</div>
    <nav class="nav"><a class="nav-link" href="#about">About</a><a class="nav-link" href="#news">News</a><a class="nav-link" href="#publications">Publications</a><a class="nav-link" href="#projects">Projects</a><a class="nav-link" href="#resources">Resources</a></nav>
  </header>
  <main>
<section class="hero" id="about">
  <div class="panel profile-card">
    <div class="avatar">
      <img src="assets/images/placeholder.svg" alt="Portrait of Xiangtao Meng" loading="lazy" />
    </div>
    <div class="profile-text">
      <p class="eyebrow">孟祥涛</p>
      <h1>Xiangtao Meng</h1>
      <p class="role">2nd-year Ph.D., School of Cyber Science and Technology · Shandong University</p>
      <p class="muted">Qingdao, China</p>
      <p class="lede">Exploring secure and trustworthy AI, from deepfake detection to robust large language models.</p>
      <div class="chips">
        <span class="chip">Email · mengxiangtao@mail.sdu.edu.cn</span>
        <a class="chip" href="https://github.com/AnonymousUserA" target="_blank" rel="noopener">GitHub</a><a class="chip" href="https://scholar.google.com/citations?hl=zh-CN&user=W_GK6gcAAAAJ" target="_blank" rel="noopener">Google Scholar</a><a class="chip" href="https://sduiseclab.github.io/" target="_blank" rel="noopener">ISecLab</a>
      </div>
      <div class="actions"><a class="btn btn-primary" href="assets/downloads/cv.pdf" target="_blank" rel="noopener">Download CV</a></div>
    </div>
  </div>
  <div class="highlights"><div class="highlight-card"><h3>Trustworthy Machine Learning</h3><p>Researching safety, robustness, and privacy across generative models and LLM agents.</p></div><div class="highlight-card"><h3>Deepfake Forensics</h3><p>Building attacks and defenses for facial forgery detection in practical pipelines.</p></div><div class="highlight-card"><h3>Secure LLM Systems</h3><p>Designing evaluation frameworks that expose risk interactions and support safer deployments.</p></div></div>
</section>
<section id="news" class="section"><div class="section-header"><p class="eyebrow">Updates</p><h2>Latest News</h2></div><div class="timeline"><article class="timeline-card"><div class="timeline-date">2025-11-03</div><div class="timeline-content"><h3>Featured by MIT Technology Review China</h3><p>Media coverage of our latest LLM defense study.</p><a href="https://wap.mittrchina.com/news/detail/15426" class="text-link">Read more</a></div></article><article class="timeline-card"><div class="timeline-date">2025-10-10</div><div class="timeline-content"><h3>Preprint: From Defender to Devil?</h3><p>Investigating unintended risk interactions introduced by LLM defenses.</p><a href="https://arxiv.org/abs/2510.07968" class="text-link">Read more</a></div></article><article class="timeline-card"><div class="timeline-date">2025-09-18</div><div class="timeline-content"><h3>ErrorTrace accepted at NeurIPS 2025 (spotlight)</h3><p>Black-box traceability based on model family error space.</p><a href="https://arxiv.org/abs/2509.06026" class="text-link">Read more</a></div></article><article class="timeline-card"><div class="timeline-date">2025-09-06</div><div class="timeline-content"><h3>Industry collaboration launched</h3><p>Joint research project on LLM security testing and risk assessment with Topsec.</p><a href="#" class="text-link">Read more</a></div></article><article class="timeline-card"><div class="timeline-date">2025-08-28</div><div class="timeline-content"><h3>Preprint: Safe-Control</h3><p>Safety patch for mitigating unsafe content in text-to-image generation models.</p><a href="https://arxiv.org/abs/2508.21099" class="text-link">Read more</a></div></article><article class="timeline-card"><div class="timeline-date">2025-08-13</div><div class="timeline-content"><h3>DCMI accepted at CCS 2025</h3><p>Differential calibration membership inference against RAG.</p><a href="https://arxiv.org/abs/2509.06026" class="text-link">Read more</a></div></article><article class="timeline-card"><div class="timeline-date">2025-03-11</div><div class="timeline-content"><h3>Fuzz-testing meets LLM-based agents accepted at IEEE S&P 2025</h3><p>Automated framework for jailbreaking text-to-image generation models.</p><a href="https://arxiv.org/abs/2408.00523" class="text-link">Read more</a></div></article><article class="timeline-card"><div class="timeline-date">2024-11-15</div><div class="timeline-content"><h3>Outstanding master's thesis</h3><p>Recognized for thesis on robustness research for deepfake detection.</p><a href="#" class="text-link">Read more</a></div></article></div></section><section id="publications" class="section"><div class="section-header"><p class="eyebrow">Selected Works</p><h2>Publications</h2></div><div class="cards-grid"><article class="card"><div class="card-meta"><span class="pill">arXiv</span><span class="pill pill-muted">2025</span></div><h3>From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses</h3><p class="muted">Xiangtao Meng, Tianshuo Cong, Li Wang, Wenyu Chen, Zheng Li✉, Shanqing Guo✉, Xiaoyun Wang✉</p><div class="chip-row"><span class="chip chip-small">LLM Safety</span><span class="chip chip-small">Risk Analysis</span></div><div class="links-row"><a href="https://arxiv.org/abs/2510.07968" class="text-link">Paper</a></div></article><article class="card"><div class="card-meta"><span class="pill">NeurIPS (Spotlight)</span><span class="pill pill-muted">2025</span></div><h3>ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space</h3><p class="muted">Chuanchao Zang, Xiangtao Meng, Wenyu Chen, Tianshuo Cong, Zha Yaxing, Dong Qi, Zheng Li, Shanqing Guo</p><div class="chip-row"><span class="chip chip-small">Model Provenance</span><span class="chip chip-small">NeurIPS 2025</span></div><div class="links-row"><a href="#" class="text-link">Link</a></div></article><article class="card"><div class="card-meta"><span class="pill">arXiv</span><span class="pill pill-muted">2025</span></div><h3>Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</h3><p class="muted">Xiangtao Meng, Yingkai Dong, Ning Yu, Li Wang, Zheng Li✉, Shanqing Guo✉</p><div class="chip-row"><span class="chip chip-small">T2I Safety</span><span class="chip chip-small">Defense</span></div><div class="links-row"><a href="https://arxiv.org/abs/2508.21099" class="text-link">Paper</a></div></article><article class="card"><div class="card-meta"><span class="pill">CCS</span><span class="pill pill-muted">2025</span></div><h3>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</h3><p class="muted">Xinyu, Xiangtao Meng✉, Yingkai Dong, Zheng Li✉, Shanqing Guo✉</p><div class="chip-row"><span class="chip chip-small">RAG Security</span><span class="chip chip-small">CCS 2025</span></div><div class="links-row"><a href="https://arxiv.org/abs/2509.06026" class="text-link">Paper</a></div></article><article class="card"><div class="card-meta"><span class="pill">IEEE S&P</span><span class="pill pill-muted">2025</span></div><h3>Fuzz-testing meets LLM-based agents: An automated and efficient framework for jailbreaking text-to-image generation models</h3><p class="muted">Yingkai Dong, Xiangtao Meng, Ning Yu, Li Wang, Zheng Li✉, Shanqing Guo✉</p><div class="chip-row"><span class="chip chip-small">Adversarial Testing</span><span class="chip chip-small">IEEE S&P</span></div><div class="links-row"><a href="https://arxiv.org/abs/2408.00523" class="text-link">Paper</a></div></article><article class="card"><div class="card-meta"><span class="pill">IEEE S&P</span><span class="pill pill-muted">2024</span></div><h3>AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection</h3><p class="muted">Xiangtao Meng, Li Wang, Shanqing Guo✉, Lei Ju, Qingchuan Zhao</p><div class="chip-row"><span class="chip chip-small">Deepfake Attack</span><span class="chip chip-small">Code Released</span></div><div class="links-row"><a href="https://arxiv.org/abs/2312.08675" class="text-link">Paper</a><a href="https://github.com/AnonymousUserA/AVA" class="text-link">Code</a></div></article><article class="card"><div class="card-meta"><span class="pill">ACM TOPS</span><span class="pill pill-muted">2024</span></div><h3>DEEPFAKER: A Unified Evaluation Platform for Facial Deepfake and Detection Models</h3><p class="muted">Li Wang, Xiangtao Meng, Dan Li, Xuhong Zhang, Shouling Ji, Shanqing Guo✉</p><div class="chip-row"><span class="chip chip-small">Benchmark</span><span class="chip chip-small">CCF B</span></div><div class="links-row"><a href="https://dl.acm.org/doi/abs/10.1145/3634914" class="text-link">Paper</a></div></article></div></section><section id="projects" class="section"><div class="section-header"><p class="eyebrow">Research & Services</p><h2>Projects</h2></div><div class="cards-grid project-grid"><article class="card project-card"><div class="project-media"><img src="assets/images/placeholder.svg" alt="LLM Security Testing Framework" loading="lazy" /></div><div class="project-body"><h3>LLM Security Testing Framework</h3><p>Joint research with Topsec to design systematic testing and risk assessment for large language models.</p><div class="chip-row"><span class="chip chip-small">LLM Security</span><span class="chip chip-small">Industry Collaboration</span></div></div></article><article class="card project-card"><div class="project-media"><img src="assets/images/placeholder.svg" alt="Safe-Control" loading="lazy" /></div><div class="project-body"><h3>Safe-Control</h3><p>Safety patch for grounding text-to-image generation within controllable boundaries.</p><div class="chip-row"><span class="chip chip-small">Defense</span><span class="chip chip-small">Generative AI</span></div></div></article><article class="card project-card"><div class="project-media"><img src="assets/images/placeholder.svg" alt="ErrorTrace" loading="lazy" /></div><div class="project-body"><h3>ErrorTrace</h3><p>Tracing model lineage through error space signatures in black-box settings.</p><div class="chip-row"><span class="chip chip-small">Traceability</span><span class="chip chip-small">NeurIPS 2025</span></div></div></article><article class="card project-card"><div class="project-media"><img src="assets/images/placeholder.svg" alt="DCMI" loading="lazy" /></div><div class="project-body"><h3>DCMI</h3><p>Membership inference against retrieval-augmented generation via differential calibration.</p><div class="chip-row"><span class="chip chip-small">Privacy</span><span class="chip chip-small">CCS 2025</span></div></div></article><article class="card project-card"><div class="project-media"><img src="assets/images/placeholder.svg" alt="DEEPFAKER" loading="lazy" /></div><div class="project-body"><h3>DEEPFAKER</h3><p>Unified evaluation platform covering facial deepfake generation and detection models.</p><div class="chip-row"><span class="chip chip-small">Benchmark</span><span class="chip chip-small">TOPS</span></div></div></article></div></section><section id="resources" class="section"><div class="section-header"><p class="eyebrow">Notes & Links</p><h2>Resources</h2></div><div class="resource-grid"><div class="resource-card"><h3>Tutorials</h3><ul><li><a href="https://www.notion.so/Ubuntu-Github-dc04221847994d65834290470e394777" target="_blank" rel="noopener">Ubuntu 利用Github配置个人主页(jemdoc)</a> <span class="muted">— 环境配置记录</span></li><li><a href="https://www.notion.so/Anaconda-21b9d59fb67146bba32feeedaadce461" target="_blank" rel="noopener">Anaconda 的使用教程</a></li><li><a href="https://www.notion.so/cc157d48f8fb4073a7432657bdb6e9d7" target="_blank" rel="noopener">安全顶会查询教程</a></li><li><a href="https://www.notion.so/add0ff2720624f5e96cee6dce0820594" target="_blank" rel="noopener">Windows 上传文件到远程服务器</a></li></ul></div><div class="resource-card"><h3>Research Notes</h3><ul><li><a href="https://www.notion.so/Kaldi-ASpIRE-34aa438f43a6449b8501df72aa14c49f" target="_blank" rel="noopener">Kaldi 安装及 ASpIRE 模型的使用</a></li><li><a href="https://www.notion.so/534a7912d8fe43498e4131ed204dc103" target="_blank" rel="noopener">语音对抗中值得收藏的文章</a></li><li><a href="https://www.notion.so/51e6c16fbaae4b5ea035b680f8e75632" target="_blank" rel="noopener">智能合约攻击方式总结</a></li><li><a href="https://www.notion.so/476fc444867c4cc4a6debd2c1d9a42ee" target="_blank" rel="noopener">物联网安全态势感知系统的研究与实现</a></li><li><a href="https://www.notion.so/IPv6-92c43a0fd43b4a21b99997231c159b57" target="_blank" rel="noopener">基于种子地址的IPv6地址探测技术</a></li><li><a href="https://www.notion.so/Intriguing-properties-of-neural-networks-L-BFGS-7a65e750d8c14f698a995d41ce95cacf" target="_blank" rel="noopener">Intriguing properties of neural networks (L-BFGS 开山之作)</a></li><li><a href="https://www.notion.so/CommanderSong-6a8ae90ba1694dcabaccf149fc4b580c" target="_blank" rel="noopener">CommanderSong 语音对抗经典之作</a></li><li><a href="https://www.notion.so/Devil-s-Whisper-A-General-Approach-for-Physical-Adversarial-Attacks-against-Commercial-Black-box-Spe-e598291f176f43e5882cdf2c28733692" target="_blank" rel="noopener">Devil's Whisper 物理语音攻击</a></li><li><a href="https://www.notion.so/4bbdf4d59eeb4b61a656178a0f1a59b8" target="_blank" rel="noopener">语音攻击类论文汇总整理</a></li><li><a href="https://www.notion.so/TxSpector-USENIX-Security-69703e1f1a7841118443232af966fd08" target="_blank" rel="noopener">TXSPECTOR: Uncovering Attacks in Ethereum from Transactions</a></li><li><a href="https://www.notion.so/VULTRON-62707143bab54c7da88bffbef5cfd1e" target="_blank" rel="noopener">VULTRON: Catching Vulnerable Smart Contracts</a></li></ul></div></div></section></main>
  <footer class="site-footer">
    <div class="footer-links"><a href="https://github.com/AnonymousUserA" target="_blank" rel="noopener">GitHub</a><a href="https://scholar.google.com/citations?hl=zh-CN&user=W_GK6gcAAAAJ" target="_blank" rel="noopener">Google Scholar</a><a href="https://sduiseclab.github.io/" target="_blank" rel="noopener">ISecLab</a></div>
    <p class="muted">Last updated with a lightweight static build pipeline.</p>
  </footer>
</body>
</html>
