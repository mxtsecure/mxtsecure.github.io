<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Xiangtao Meng | Trustworthy ML</title>
  <meta name="description" content="Researcher in trustworthy machine learning, focusing on deepfake forensics and LLM safety." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Source+Serif+4:opsz,wght@8..60,600;8..60,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="assets/styles.css" />
</head>
<body data-theme="dark">
  <main class="page-shell">
    
<header class="hero">
  <div class="hero-card">
    <div class="hero-grid">
      <div class="hero-avatar"><img src="assets/images/placeholder.svg" alt="Portrait of Xiangtao Meng" loading="lazy"></div>
      <div>
        <div class="hero-meta">
          <div class="name-block">
            <p class="eyebrow">孟祥涛</p>
            <h1>Xiangtao Meng</h1>
          </div>
          <div class="role-block">
            <p class="role">2nd-year Ph.D., School of Cyber Science and Technology</p>
            <p class="muted">Shandong University</p>
            <p class="muted">Qingdao, China</p>
          </div>
        </div>
        <p class="lede">Exploring secure and trustworthy AI, from deepfake detection to robust large language models.</p>
        <div class="contact-row">
          <a class="inline-link" href="mailto:mengxiangtao@mail.sdu.edu.cn">mengxiangtao@mail.sdu.edu.cn</a>
          <a class="inline-link" href="https://github.com/AnonymousUserA" target="_blank" rel="noopener">GitHub</a><a class="inline-link" href="https://scholar.google.com/citations?hl=zh-CN&user=W_GK6gcAAAAJ" target="_blank" rel="noopener">Google Scholar</a><a class="inline-link" href="https://sduiseclab.github.io/" target="_blank" rel="noopener">ISecLab</a>
        </div>
        <div class="actions"><a class="inline-button" href="assets/downloads/cv.pdf" target="_blank" rel="noopener">Download CV</a></div>
      </div>
    </div>
    <div class="hero-highlights"><div class="highlight-card"><h3>Trustworthy Machine Learning</h3><p>Researching safety, robustness, and privacy across generative models and LLM agents.</p></div><div class="highlight-card"><h3>Deepfake Forensics</h3><p>Building attacks and defenses for facial forgery detection in practical pipelines.</p></div><div class="highlight-card"><h3>Secure LLM Systems</h3><p>Designing evaluation frameworks that expose risk interactions and support safer deployments.</p></div></div>
  </div>
  <nav class="tab-nav" aria-label="Primary"><a class="tab-link" href="#about">About</a><a class="tab-link" href="#news">News</a><a class="tab-link" href="#publications">Publications</a><a class="tab-link" href="#projects">Projects</a><a class="tab-link" href="#resources">Resources</a></nav>
</header>
<div class="content">
<section id="about" class="section card-section">
  <div class="section-header"><p class="eyebrow">Bio</p><div class="section-title"><h2>Profile</h2><span class="divider"></span></div></div>
  <div class="bio-grid">
    <div class="bio-summary">
      <p class="lede">Exploring secure and trustworthy AI, from deepfake detection to robust large language models.</p>
      <p class="muted">Based in Qingdao, China. 2nd-year Ph.D., School of Cyber Science and Technology at Shandong University.</p>
    </div>
    <ul class="focus-list"><li><div class="item-title">Trustworthy Machine Learning</div><p class="muted">Researching safety, robustness, and privacy across generative models and LLM agents.</p></li><li><div class="item-title">Deepfake Forensics</div><p class="muted">Building attacks and defenses for facial forgery detection in practical pipelines.</p></li><li><div class="item-title">Secure LLM Systems</div><p class="muted">Designing evaluation frameworks that expose risk interactions and support safer deployments.</p></li></ul>
  </div>
</section>
<section id="news" class="section card-section"><div class="section-header"><p class="eyebrow">Updates</p><div class="section-title"><h2>Latest News</h2><span class="divider"></span></div></div><div class="list-stack">
<div class="list-row">
  <div class="list-label">2025-11-03</div>
  <div class="list-body">
    <div class="item-title">Featured by MIT Technology Review China</div>
    <p class="muted">Media coverage of our latest LLM defense study.</p>
    <a href="https://wap.mittrchina.com/news/detail/15426" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025-10-10</div>
  <div class="list-body">
    <div class="item-title">Preprint: From Defender to Devil?</div>
    <p class="muted">Investigating unintended risk interactions introduced by LLM defenses.</p>
    <a href="https://arxiv.org/abs/2510.07968" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025-09-18</div>
  <div class="list-body">
    <div class="item-title">ErrorTrace accepted at NeurIPS 2025 (spotlight)</div>
    <p class="muted">Black-box traceability based on model family error space.</p>
    <a href="https://arxiv.org/abs/2509.06026" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025-09-06</div>
  <div class="list-body">
    <div class="item-title">Industry collaboration launched</div>
    <p class="muted">Joint research project on LLM security testing and risk assessment with Topsec.</p>
    <a href="#" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025-08-28</div>
  <div class="list-body">
    <div class="item-title">Preprint: Safe-Control</div>
    <p class="muted">Safety patch for mitigating unsafe content in text-to-image generation models.</p>
    <a href="https://arxiv.org/abs/2508.21099" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025-08-13</div>
  <div class="list-body">
    <div class="item-title">DCMI accepted at CCS 2025</div>
    <p class="muted">Differential calibration membership inference against RAG.</p>
    <a href="https://arxiv.org/abs/2509.06026" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025-03-11</div>
  <div class="list-body">
    <div class="item-title">Fuzz-testing meets LLM-based agents accepted at IEEE S&P 2025</div>
    <p class="muted">Automated framework for jailbreaking text-to-image generation models.</p>
    <a href="https://arxiv.org/abs/2408.00523" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2024-11-15</div>
  <div class="list-body">
    <div class="item-title">Outstanding master's thesis</div>
    <p class="muted">Recognized for thesis on robustness research for deepfake detection.</p>
    <a href="#" class="inline-link" target="_blank" rel="noopener">Read more</a>
  </div>
</div>
</div></section><section id="publications" class="section card-section"><div class="section-header"><p class="eyebrow">Selected Works</p><div class="section-title"><h2>Publications</h2><span class="divider"></span></div></div><div class="list-stack">
<div class="list-row">
  <div class="list-label">2025</div>
  <div class="list-body">
    <div class="item-title">From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses</div>
    <div class="muted">Xiangtao Meng, Tianshuo Cong, Li Wang, Wenyu Chen, Zheng Li✉, Shanqing Guo✉, Xiaoyun Wang✉</div>
    <div class="meta">arXiv · <span class="tag">LLM Safety</span> <span class="tag">Risk Analysis</span></div>
    <div class="links-row"><a href="https://arxiv.org/abs/2510.07968" class="inline-link" target="_blank" rel="noopener">Paper</a></div>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025</div>
  <div class="list-body">
    <div class="item-title">ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space</div>
    <div class="muted">Chuanchao Zang, Xiangtao Meng, Wenyu Chen, Tianshuo Cong, Zha Yaxing, Dong Qi, Zheng Li, Shanqing Guo</div>
    <div class="meta">NeurIPS (Spotlight) · <span class="tag">Model Provenance</span> <span class="tag">NeurIPS 2025</span></div>
    <div class="links-row"><a href="#" class="inline-link" target="_blank" rel="noopener">Link</a></div>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025</div>
  <div class="list-body">
    <div class="item-title">Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</div>
    <div class="muted">Xiangtao Meng, Yingkai Dong, Ning Yu, Li Wang, Zheng Li✉, Shanqing Guo✉</div>
    <div class="meta">arXiv · <span class="tag">T2I Safety</span> <span class="tag">Defense</span></div>
    <div class="links-row"><a href="https://arxiv.org/abs/2508.21099" class="inline-link" target="_blank" rel="noopener">Paper</a></div>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025</div>
  <div class="list-body">
    <div class="item-title">DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</div>
    <div class="muted">Xinyu, Xiangtao Meng✉, Yingkai Dong, Zheng Li✉, Shanqing Guo✉</div>
    <div class="meta">CCS · <span class="tag">RAG Security</span> <span class="tag">CCS 2025</span></div>
    <div class="links-row"><a href="https://arxiv.org/abs/2509.06026" class="inline-link" target="_blank" rel="noopener">Paper</a></div>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2025</div>
  <div class="list-body">
    <div class="item-title">Fuzz-testing meets LLM-based agents: An automated and efficient framework for jailbreaking text-to-image generation models</div>
    <div class="muted">Yingkai Dong, Xiangtao Meng, Ning Yu, Li Wang, Zheng Li✉, Shanqing Guo✉</div>
    <div class="meta">IEEE S&P · <span class="tag">Adversarial Testing</span> <span class="tag">IEEE S&P</span></div>
    <div class="links-row"><a href="https://arxiv.org/abs/2408.00523" class="inline-link" target="_blank" rel="noopener">Paper</a></div>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2024</div>
  <div class="list-body">
    <div class="item-title">AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection</div>
    <div class="muted">Xiangtao Meng, Li Wang, Shanqing Guo✉, Lei Ju, Qingchuan Zhao</div>
    <div class="meta">IEEE S&P · <span class="tag">Deepfake Attack</span> <span class="tag">Code Released</span></div>
    <div class="links-row"><a href="https://arxiv.org/abs/2312.08675" class="inline-link" target="_blank" rel="noopener">Paper</a> <a href="https://github.com/AnonymousUserA/AVA" class="inline-link" target="_blank" rel="noopener">Code</a></div>
  </div>
</div>

<div class="list-row">
  <div class="list-label">2024</div>
  <div class="list-body">
    <div class="item-title">DEEPFAKER: A Unified Evaluation Platform for Facial Deepfake and Detection Models</div>
    <div class="muted">Li Wang, Xiangtao Meng, Dan Li, Xuhong Zhang, Shouling Ji, Shanqing Guo✉</div>
    <div class="meta">ACM TOPS · <span class="tag">Benchmark</span> <span class="tag">CCF B</span></div>
    <div class="links-row"><a href="https://dl.acm.org/doi/abs/10.1145/3634914" class="inline-link" target="_blank" rel="noopener">Paper</a></div>
  </div>
</div>
</div></section><section id="projects" class="section card-section"><div class="section-header"><p class="eyebrow">Research & Services</p><div class="section-title"><h2>Projects</h2><span class="divider"></span></div></div><div class="list-stack">
<div class="list-row project-row">
  <div class="thumb"><img src="assets/images/placeholder.svg" alt="LLM Security Testing Framework" loading="lazy"></div>
  <div class="list-body">
    <div class="item-title">LLM Security Testing Framework</div>
    <p class="muted">Joint research with Topsec to design systematic testing and risk assessment for large language models.</p>
    <div class="meta"><span class="tag">LLM Security</span> <span class="tag">Industry Collaboration</span></div>
  </div>
</div>

<div class="list-row project-row">
  <div class="thumb"><img src="assets/images/placeholder.svg" alt="Safe-Control" loading="lazy"></div>
  <div class="list-body">
    <div class="item-title">Safe-Control</div>
    <p class="muted">Safety patch for grounding text-to-image generation within controllable boundaries.</p>
    <div class="meta"><span class="tag">Defense</span> <span class="tag">Generative AI</span></div>
  </div>
</div>

<div class="list-row project-row">
  <div class="thumb"><img src="assets/images/placeholder.svg" alt="ErrorTrace" loading="lazy"></div>
  <div class="list-body">
    <div class="item-title">ErrorTrace</div>
    <p class="muted">Tracing model lineage through error space signatures in black-box settings.</p>
    <div class="meta"><span class="tag">Traceability</span> <span class="tag">NeurIPS 2025</span></div>
  </div>
</div>

<div class="list-row project-row">
  <div class="thumb"><img src="assets/images/placeholder.svg" alt="DCMI" loading="lazy"></div>
  <div class="list-body">
    <div class="item-title">DCMI</div>
    <p class="muted">Membership inference against retrieval-augmented generation via differential calibration.</p>
    <div class="meta"><span class="tag">Privacy</span> <span class="tag">CCS 2025</span></div>
  </div>
</div>

<div class="list-row project-row">
  <div class="thumb"><img src="assets/images/placeholder.svg" alt="DEEPFAKER" loading="lazy"></div>
  <div class="list-body">
    <div class="item-title">DEEPFAKER</div>
    <p class="muted">Unified evaluation platform covering facial deepfake generation and detection models.</p>
    <div class="meta"><span class="tag">Benchmark</span> <span class="tag">TOPS</span></div>
  </div>
</div>
</div></section><section id="resources" class="section card-section"><div class="section-header"><p class="eyebrow">Notes & Links</p><div class="section-title"><h2>Resources</h2><span class="divider"></span></div></div><div class="resource-list"><div class="resource-block"><div class="item-title">Tutorials</div><ul><li><a class="inline-link" href="https://www.notion.so/Ubuntu-Github-dc04221847994d65834290470e394777" target="_blank" rel="noopener">Ubuntu 利用Github配置个人主页(jemdoc)</a> <span class="muted">— 环境配置记录</span></li><li><a class="inline-link" href="https://www.notion.so/Anaconda-21b9d59fb67146bba32feeedaadce461" target="_blank" rel="noopener">Anaconda 的使用教程</a></li><li><a class="inline-link" href="https://www.notion.so/cc157d48f8fb4073a7432657bdb6e9d7" target="_blank" rel="noopener">安全顶会查询教程</a></li><li><a class="inline-link" href="https://www.notion.so/add0ff2720624f5e96cee6dce0820594" target="_blank" rel="noopener">Windows 上传文件到远程服务器</a></li></ul></div><div class="resource-block"><div class="item-title">Research Notes</div><ul><li><a class="inline-link" href="https://www.notion.so/Kaldi-ASpIRE-34aa438f43a6449b8501df72aa14c49f" target="_blank" rel="noopener">Kaldi 安装及 ASpIRE 模型的使用</a></li><li><a class="inline-link" href="https://www.notion.so/534a7912d8fe43498e4131ed204dc103" target="_blank" rel="noopener">语音对抗中值得收藏的文章</a></li><li><a class="inline-link" href="https://www.notion.so/51e6c16fbaae4b5ea035b680f8e75632" target="_blank" rel="noopener">智能合约攻击方式总结</a></li><li><a class="inline-link" href="https://www.notion.so/476fc444867c4cc4a6debd2c1d9a42ee" target="_blank" rel="noopener">物联网安全态势感知系统的研究与实现</a></li><li><a class="inline-link" href="https://www.notion.so/IPv6-92c43a0fd43b4a21b99997231c159b57" target="_blank" rel="noopener">基于种子地址的IPv6地址探测技术</a></li><li><a class="inline-link" href="https://www.notion.so/Intriguing-properties-of-neural-networks-L-BFGS-7a65e750d8c14f698a995d41ce95cacf" target="_blank" rel="noopener">Intriguing properties of neural networks (L-BFGS 开山之作)</a></li><li><a class="inline-link" href="https://www.notion.so/CommanderSong-6a8ae90ba1694dcabaccf149fc4b580c" target="_blank" rel="noopener">CommanderSong 语音对抗经典之作</a></li><li><a class="inline-link" href="https://www.notion.so/Devil-s-Whisper-A-General-Approach-for-Physical-Adversarial-Attacks-against-Commercial-Black-box-Spe-e598291f176f43e5882cdf2c28733692" target="_blank" rel="noopener">Devil's Whisper 物理语音攻击</a></li><li><a class="inline-link" href="https://www.notion.so/4bbdf4d59eeb4b61a656178a0f1a59b8" target="_blank" rel="noopener">语音攻击类论文汇总整理</a></li><li><a class="inline-link" href="https://www.notion.so/TxSpector-USENIX-Security-69703e1f1a7841118443232af966fd08" target="_blank" rel="noopener">TXSPECTOR: Uncovering Attacks in Ethereum from Transactions</a></li><li><a class="inline-link" href="https://www.notion.so/VULTRON-62707143bab54c7da88bffbef5cfd1e" target="_blank" rel="noopener">VULTRON: Catching Vulnerable Smart Contracts</a></li></ul></div></div></section></div>
  </main>
  <footer class="site-footer">
    <div class="footer-links"><a href="https://github.com/AnonymousUserA" target="_blank" rel="noopener">GitHub</a><a href="https://scholar.google.com/citations?hl=zh-CN&user=W_GK6gcAAAAJ" target="_blank" rel="noopener">Google Scholar</a><a href="https://sduiseclab.github.io/" target="_blank" rel="noopener">ISecLab</a></div>
    <p class="muted">Last updated with a lightweight static build pipeline.</p>
  </footer>
</body>
</html>
