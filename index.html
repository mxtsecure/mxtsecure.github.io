<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiangtao Meng's Homepage</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Xiangtao Meng's Homepage</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photos/mengxiangtao.jpg" alt="alt text" width="148px" height="200px" />&nbsp;</td>
<td align="left"><h2>Xiangtao Meng 孟祥涛</h2>
<p>2th-year Ph.D, <a href="https://sduiseclab.github.io/">ISecLab</a> <br /> <br />
School of Cyber Science and Technology, Shandong University <br /><br /> 
E-mail: <i>mengxiangtao AT mail.sdu.edu.cn</i> <br /> <br />
[<a href="https://github.com/AnonymousUserA">Github</a>] [<a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=W_GK6gcAAAAJ">Google Scholar</a>] </p>
</td></tr></table>
<div class="infoblock">
<div class="blockcontent">
<p><b>News:</b></p>
<ul>
<li><p>2025-11-3: Our work was featured by <b>MIT Technology Review China</b> (<a href="https://wap.mittrchina.com/news/detail/15426">article</a>; paper: <a href="https://arxiv.org/abs/2508.21099">arXiv</a>).</p>
</li>
<li><p>2025-10-10: Preprint published: &ldquo;From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses.&rdquo;</p>
</li>
<li><p>2025-9-18: One paper titled &ldquo;ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space&rdquo; got accepted in <b>NeurIPS 2025 (spotlight)</b>! Congratulations to Chuanchao!!!</p>
</li>
<li><p>2025-9-6: Funded by Topsec (Tianrongxin), I started the joint research project <b>“Design of Security Testing Framework and Risk Assessment for Large Language Models”</b>.</p>
</li>
<li><p>2025-8-28: Preprint published: <b>“Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models”</b></p>
</li>
<li><p>2025-8-13: One paper titled &ldquo;DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation&rdquo; got accepted in <b>CCS 2025</b>! Congratulations to Xinyu!!! First time as a <b>corresponding author</b>.</p>
</li>
<li><p>2025-3-11: One paper titled &ldquo;Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-To-Image Generation Models&rdquo; got accepted in IEEE S&amp;P 2025! Congratulations to Yingkai!!!</p>
</li>
<li><p>2024-11-15: My master's thesis, titled &ldquo;Robustness Research on Deepfake Detection Technology,&rdquo; has been recognized as an outstanding master's thesis at Shandong University.</p>
</li>
<li><p>2023-11-12: One paper titled “AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection” got accepted in IEEE S&amp;P 2024!</p>
</li>
<li><p>2023-10-13: One paper titled “DEEPFAKER: A Unified Evaluation Platform for Facial Deepfake and Detection Models” got accepted in TOPS(ACM Transactions on Privacy and Security) 2024!</p>
</li>
</ul>
</div></div>
<h2>Education</h2>
<ul>
<li><p>Sep 2023 - Present: Ph.D，in School of Cyber Science and Technology, Shandong University，supervised by <a href="https://www.ias.tsinghua.edu.cn/en/info/1059/1173.htm">Prof. Xiaoyun Wang</a> and <a href="https://faculty.sdu.edu.cn/guoshanqing/zh_CN/index.htm">Prof. Shanqing Guo</a>, Qingdao, China.</p>
</li>
<li><p>Sep 2020 - Jun 2023: M.Sc. in School of Cyber Science and Technology, Shandong University，supervised by<a href="https://faculty.sdu.edu.cn/guoshanqing/zh_CN/index.htm">Prof. Shanqing Guo</a>, Qingdao, China.</p>
</li>
<li><p>Sep 2016 - Jun 2020: B.Sc. in Network Engineering, Shandong University of Science and Technology, Qingdao, China.</p>
</li>
</ul>
<h2>Research Interests</h2>
<p>My research centers on Trustworthy Machine Learning (expercially DeepFake), including disclosing the safety, security and privacy of the Machine Learning (such as, Text-to-Image, ChatGPT, etc.) and proposing corresponding defense measures.</p>
<h2>Publication</h2>
<ol>
<li><p><b>Xiangtao Meng</b>, Tianshuo Cong, Li Wang, Wenyu Chen, Zheng Li✉, Shanqing Guo✉, Xiaoyun Wang✉. From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses. arXiv 2025. [<a href="https://arxiv.org/abs/2510.07968">PDF</a>]</p>
</li>
<li><p>Chuanchao Zang, <b>Xiangtao Meng</b>, Wenyu Chen, Tianshuo Cong, Zha Yaxing, Dong Qi, Zheng Li, Shanqing Guo. ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space. NeurIPS 2025 (spotlight). <a href="TOP">TOP</a> <a href="*CCF">A*</a></p>
</li>
<li><p><b>Xiangtao Meng</b>, Yingkai Dong, Ning Yu, Li Wang, Zheng Li✉, Shanqing Guo✉. Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models. arXiv 2025. [<a href="https://arxiv.org/abs/2508.21099">PDF</a>]</p>
</li>
<li><p>Xinyu, <b>Xiangtao Meng</b>✉, Yingkai Dong, Zheng Li✉, Shanqing Guo✉. DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation. CCS 2025. <a href="TOP">TOP</a> <a href="*CCF">A*</a> [<a href="https://arxiv.org/abs/2509.06026">PDF</a>]</p>
</li>
<li><p>Yingkai Dong, <b>Xiangtao Meng</b>, Ning Yu, Zheng Li✉, Shanqing Guo✉. Fuzz-testing meets llm-based agents: An automated and efficient framework for jailbreaking text-to-image generation models. IEEE S&amp;P 2025. <a href="TOP">TOP</a> <a href="*CCF">A*</a> arxiv. [<a href="https://arxiv.org/abs/2408.00523">PDF</a>]</p>
</li>
<li><p><b>Xiangtao Meng</b>, Li Wang, Shanqing Guo✉, Lei Ju, Qingchuan Zhao. AVA: Inconspicuous Attribute Variation-based Adversarial Attack bypassing DeepFake Detection. IEEE S&amp;P 2024. <a href="TOP">TOP</a> <a href="*CCF">A*</a> [<a href="https://arxiv.org/abs/2312.08675">PDF</a>] [<a href="https://github.com/AnonymousUserA/AVA">Code</a>]</p>
</li>
<li><p>Li Wang, <b>Xiangtao Meng</b>, Dan Li, Xuhong Zhang, Shouling Ji, Shanqing Guo✉. DEEPFAKER: A Unified Evaluation Platform for Facial Deepfake and Detection Models. ACM Transactions on Privacy and Security (TOPS) 2024. <a href="*CCF">B*</a> [<a href="https://dl.acm.org/doi/abs/10.1145/3634914">PDF</a>]</p>
</li>
</ol>
<p>✉ Corresponding author.</p>
<h2>Projects</h2>
<ul>
<li><p>Funded by Topsec (Tianrongxin), I am jointly conducting the research project <b>“Design of Security Testing Framework and Risk Assessment for Large Language Models”</b>.  
This project focuses on the safety and risk management of LLMs in real-world applications, aiming to develop systematic testing methodologies and evaluation frameworks.</p>
</li>
</ul>
</div>
</body>
</html>
