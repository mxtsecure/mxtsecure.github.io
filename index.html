<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiangtao Meng's Homepage</title>
</head>
<body>
<div id="layout-content" class="site-container">
  <div id="toptitle">
    <h1>Xiangtao Meng's Homepage</h1>
  </div>

  <header class="hero-card">
    <img src="photos/mengxiangtao.jpg" alt="Portrait of Xiangtao Meng" class="hero-photo" />
    <div class="hero-content">
      <p class="hero-eyebrow">Trustworthy Machine Learning · Generative AI Security</p>
      <h2>Xiangtao Meng 孟祥涛</h2>
      <p class="hero-role">2<sup>nd</sup>-year Ph.D. Candidate, <a href="https://sduiseclab.github.io/">ISecLab</a></p>
      <p>School of Cyber Science and Technology, Shandong University<br />
      Qingdao, China</p>
      <p><strong>E-mail:</strong> <i>mengxiangtao AT mail.sdu.edu.cn</i></p>
      <div class="hero-links">
        <a class="btn" href="https://github.com/AnonymousUserA" target="_blank" rel="noreferrer">Github</a>
        <a class="btn btn-secondary" href="https://scholar.google.com/citations?hl=zh-CN&amp;user=W_GK6gcAAAAJ" target="_blank" rel="noreferrer">Google Scholar</a>
      </div>
    </div>
  </header>

  <section class="card-section">
    <div class="section-header">
      <h2>News</h2>
      <p>Recent highlights and milestones</p>
    </div>
    <ul class="news-board">
      <li class="news-item">
        <span class="news-date-badge">Nov 03, 2025</span>
        <p>Our work was featured by <strong>MIT Technology Review China</strong> (<a href="https://wap.mittrchina.com/news/detail/15426">article</a>; paper: <a href="https://arxiv.org/abs/2508.21099">arXiv</a>).</p>
      </li>
      <li class="news-item">
        <span class="news-date-badge">Oct 10, 2025</span>
        <p>Preprint published: “From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses.”</p>
      </li>
      <li class="news-item">
        <span class="news-date-badge">Sep 18, 2025</span>
        <p>ErrorTrace spotlight paper accepted to <strong>NeurIPS 2025</strong> – congratulations to Chuanchao!</p>
      </li>
      <li class="news-item">
        <span class="news-date-badge">Sep 06, 2025</span>
        <p>Launched the Topsec-funded project on <strong>security testing frameworks for large language models</strong>.</p>
      </li>
      <li class="news-item">
        <span class="news-date-badge">Aug 28, 2025</span>
        <p>Released the Safe-Control preprint on mitigating unsafe content in text-to-image models.</p>
      </li>
      <li class="news-item">
        <span class="news-date-badge">Aug 13, 2025</span>
        <p>DCMI accepted to <strong>CCS 2025</strong> – first paper with me serving as corresponding author.</p>
      </li>
    </ul>
  </section>

  <section class="card-section">
    <div class="section-header">
      <h2>Education</h2>
      <p>Academic training</p>
    </div>
    <ul class="styled-list">
      <li><strong>Sep 2023 – Present:</strong> Ph.D., School of Cyber Science and Technology, Shandong University, supervised by <a href="https://www.ias.tsinghua.edu.cn/en/info/1059/1173.htm">Prof. Xiaoyun Wang</a> and <a href="https://faculty.sdu.edu.cn/guoshanqing/zh_CN/index.htm">Prof. Shanqing Guo</a>, Qingdao, China.</li>
      <li><strong>Sep 2020 – Jun 2023:</strong> M.Sc., School of Cyber Science and Technology, Shandong University, supervised by <a href="https://faculty.sdu.edu.cn/guoshanqing/zh_CN/index.htm">Prof. Shanqing Guo</a>, Qingdao, China.</li>
      <li><strong>Sep 2016 – Jun 2020:</strong> B.Sc., Network Engineering, Shandong University of Science and Technology, Qingdao, China.</li>
    </ul>
  </section>

  <section class="card-section">
    <div class="section-header">
      <h2>Research Interests</h2>
      <p>Trustworthy ML · Generative AI Safety · Adversarial Robustness</p>
    </div>
    <p>My research focuses on trustworthy machine learning, with an emphasis on deepfake forensics, red teaming for text-to-image and LLM systems, and defense mechanisms that improve safety, security, and privacy in real-world deployments.</p>
  </section>

  <section class="publications-section">
    <div class="section-header">
      <h2>Selected Publications</h2>
      <p>Each card features a visual highlight from the work.</p>
    </div>
    <div class="author-legend">
      <span class="author-self">My name</span>
      <span class="author-corresponding">Corresponding author</span>
    </div>
    <div class="publication-grid">
      <article class="publication-card">
        <img src="photos/chuanchuan2.jpg" alt="Visualization for From Defender to Devil" class="publication-thumb" />
        <div class="publication-body">
          <div class="paper-topline">
            <span class="venue-badge">arXiv 2025</span>
          </div>
          <h3>From Defender to Devil? Unintended Risk Interactions Induced by LLM Defenses</h3>
          <p class="authors"><span class="author-self">Xiangtao Meng</span>, Tianshuo Cong, Li Wang, Wenyu Chen, <span class="author-corresponding">Zheng Li✉</span>, <span class="author-corresponding">Shanqing Guo✉</span>, <span class="author-corresponding">Xiaoyun Wang✉</span></p>
          <p class="paper-summary">We analyze how layered defenses for large language models can interact to create new attack surfaces and propose diagnostic tools for auditing compound safeguards.</p>
          <div class="paper-links">
            <a class="btn-link" href="https://arxiv.org/abs/2510.07968">PDF</a>
          </div>
        </div>
      </article>

      <article class="publication-card">
        <img src="photos/chuanchuan4.jpg" alt="Visualization for ErrorTrace" class="publication-thumb" />
        <div class="publication-body">
          <div class="paper-topline">
            <span class="venue-badge">NeurIPS 2025 (Spotlight)</span>
            <div class="paper-tags">
              <span class="paper-badge top">TOP</span>
              <span class="paper-badge ccf">CCF A</span>
            </div>
          </div>
          <h3>ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space</h3>
          <p class="authors">Chuanchao Zang, <span class="author-self">Xiangtao Meng</span>, Wenyu Chen, Tianshuo Cong, Zha Yaxing, Dong Qi, Zheng Li, Shanqing Guo</p>
          <p class="paper-summary">Introduces a model-family error space to trace suspicious predictions without accessing proprietary weights, enabling accountability across large-scale deployments.</p>
        </div>
      </article>

      <article class="publication-card">
        <img src="photos/chuanchuan3.jpg" alt="Visualization for Safe-Control" class="publication-thumb" />
        <div class="publication-body">
          <div class="paper-topline">
            <span class="venue-badge">arXiv 2025</span>
          </div>
          <h3>Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</h3>
          <p class="authors"><span class="author-self">Xiangtao Meng</span>, Yingkai Dong, Ning Yu, Li Wang, <span class="author-corresponding">Zheng Li✉</span>, <span class="author-corresponding">Shanqing Guo✉</span></p>
          <p class="paper-summary">Demonstrates lightweight safety patches that filter unsafe prompts while retaining image fidelity in diffusion-based generators.</p>
          <div class="paper-links">
            <a class="btn-link" href="https://arxiv.org/abs/2508.21099">PDF</a>
          </div>
        </div>
      </article>

      <article class="publication-card">
        <img src="photos/chuanchuan5.jpg" alt="Visualization for DCMI" class="publication-thumb" />
        <div class="publication-body">
          <div class="paper-topline">
            <span class="venue-badge">CCS 2025</span>
            <div class="paper-tags">
              <span class="paper-badge top">TOP</span>
              <span class="paper-badge ccf">CCF A</span>
            </div>
          </div>
          <h3>DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</h3>
          <p class="authors">Xinyu, <span class="author-self">Xiangtao Meng</span><span class="author-corresponding">✉</span>, Yingkai Dong, <span class="author-corresponding">Zheng Li✉</span>, <span class="author-corresponding">Shanqing Guo✉</span></p>
          <p class="paper-summary">Presents a membership inference attack tailored to RAG pipelines by exploiting calibration inconsistencies between retrieved facts and generated responses.</p>
          <div class="paper-links">
            <a class="btn-link" href="https://arxiv.org/abs/2509.06026">PDF</a>
          </div>
        </div>
      </article>

      <article class="publication-card">
        <img src="photos/chuanchuan6.jpg" alt="Visualization for Fuzz-testing meets LLM-based agents" class="publication-thumb" />
        <div class="publication-body">
          <div class="paper-topline">
            <span class="venue-badge">IEEE S&amp;P 2025</span>
            <div class="paper-tags">
              <span class="paper-badge top">TOP</span>
              <span class="paper-badge ccf">CCF A</span>
            </div>
          </div>
          <h3>Fuzz-Testing Meets LLM-Based Agents</h3>
          <p class="authors">Yingkai Dong, <span class="author-self">Xiangtao Meng</span>, Ning Yu, <span class="author-corresponding">Zheng Li✉</span>, <span class="author-corresponding">Shanqing Guo✉</span></p>
          <p class="paper-summary">Builds an automated jailbreaking and evaluation framework for text-to-image agents using guided fuzzing and behavior clustering.</p>
          <div class="paper-links">
            <a class="btn-link" href="https://arxiv.org/abs/2408.00523">PDF</a>
          </div>
        </div>
      </article>

      <article class="publication-card">
        <img src="photos/kebo.jpg" alt="Visualization for AVA" class="publication-thumb" />
        <div class="publication-body">
          <div class="paper-topline">
            <span class="venue-badge">IEEE S&amp;P 2024</span>
            <div class="paper-tags">
              <span class="paper-badge top">TOP</span>
              <span class="paper-badge ccf">CCF A</span>
            </div>
          </div>
          <h3>AVA: Inconspicuous Attribute Variation-based Adversarial Attack Bypassing DeepFake Detection</h3>
          <p class="authors"><span class="author-self">Xiangtao Meng</span>, Li Wang, <span class="author-corresponding">Shanqing Guo✉</span>, Lei Ju, Qingchuan Zhao</p>
          <p class="paper-summary">Creates stealthy attribute-variation attacks that fool state-of-the-art deepfake detectors while preserving human realism.</p>
          <div class="paper-links">
            <a class="btn-link" href="https://arxiv.org/abs/2312.08675">PDF</a>
            <a class="btn-link" href="https://github.com/AnonymousUserA/AVA">Code</a>
          </div>
        </div>
      </article>

      <article class="publication-card">
        <img src="photos/2020-3-30.jpg" alt="Visualization for DEEPFAKER" class="publication-thumb" />
        <div class="publication-body">
          <div class="paper-topline">
            <span class="venue-badge">ACM TOPS 2024</span>
            <div class="paper-tags">
              <span class="paper-badge ccf">CCF B</span>
            </div>
          </div>
          <h3>DEEPFAKER: A Unified Evaluation Platform for Facial Deepfake and Detection Models</h3>
          <p class="authors">Li Wang, <span class="author-self">Xiangtao Meng</span>, Dan Li, Xuhong Zhang, Shouling Ji, <span class="author-corresponding">Shanqing Guo✉</span></p>
          <p class="paper-summary">Provides an end-to-end benchmarking platform that compares face forgery generation pipelines and detection algorithms under consistent protocols.</p>
          <div class="paper-links">
            <a class="btn-link" href="https://dl.acm.org/doi/abs/10.1145/3634914">PDF</a>
          </div>
        </div>
      </article>
    </div>
    <p class="corresponding-note">✉ Corresponding author.</p>
  </section>

  <section class="card-section">
    <div class="section-header">
      <h2>Projects</h2>
      <p>Industry collaborations</p>
    </div>
    <div class="project-card">
      <h3>Design of Security Testing Framework and Risk Assessment for Large Language Models</h3>
      <p>Funded by Topsec (Tianrongxin). I lead the technical design of a risk evaluation framework that unifies adversarial probing, coverage metrics, and governance guidelines for enterprise LLM deployments.</p>
    </div>
  </section>
</div>
</body>
</html>
